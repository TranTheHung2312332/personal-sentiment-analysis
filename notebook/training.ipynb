{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58bb2223",
   "metadata": {},
   "source": [
    "### Import FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bec01a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\"../model/embedding.pt\", map_location=\"cpu\")\n",
    "\n",
    "embedding_matrix = ckpt[\"embedding\"]\n",
    "word2idx = ckpt[\"word2idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d29ed90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47041, 300)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, embed_dim = embedding_matrix.shape\n",
    "vocab_size, embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87718a7",
   "metadata": {},
   "source": [
    "### Declare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfc4ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, word2idx, max_len=128):\n",
    "        self.df = dataframe\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        patterns = [\n",
    "            r\"\\[[A-Z_]+\\]\",\n",
    "            r\"<\\/?[\\w_]+>\",\n",
    "            r\"\\w+\",\n",
    "            r\"[?!]{2,}\",\n",
    "            r\"\\.{3,}\",\n",
    "            r\"[^\\w\\s]\"\n",
    "        ]\n",
    "\n",
    "        combined = re.compile(\"|\".join(patterns), re.UNICODE)\n",
    "\n",
    "        return combined.findall(text)\n",
    "\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        ids = [self.word2idx.get(token, self.word2idx['<unk>']) for token in tokens]\n",
    "        ids = ids[:self.max_len]\n",
    "\n",
    "        return ids + [self.word2idx['<pad>']] * (self.max_len - len(ids))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.loc[index]\n",
    "\n",
    "        text_ids = torch.tensor(self.encode_text(row['text']))\n",
    "        extra_feats = torch.tensor([row[\"ex_intensity\"],\n",
    "            row[\"emoji_score\"],\n",
    "            row[\"all_uppercase\"],\n",
    "            row[\"uppercase_ratio\"]]\n",
    "        , dtype=torch.float32)\n",
    "\n",
    "        return text_ids, extra_feats, torch.tensor(int(row['label']), dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6e22b",
   "metadata": {},
   "source": [
    "### Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98a42f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_matrix, lstm_hidden=128, lstm_layers=1, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), padding_idx=0, freeze=False)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_matrix.size(1),\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden * 2 + 4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, text_ids, extra_feats):\n",
    "        x = self.embedding(text_ids)\n",
    "\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h = torch.cat([h[-2], h[-1]], dim=1) # BiLSTM\n",
    "\n",
    "        features = torch.cat([h, extra_feats], dim=1)\n",
    "\n",
    "        logits = self.mlp(features)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0cc93",
   "metadata": {},
   "source": [
    "### Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b7ee3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../data/preprocessed/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b024680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = SentimentDataset(train_df, word2idx, max_len=MAX_LEN)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ababea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_716\\2319374189.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), padding_idx=0, freeze=False)\n"
     ]
    }
   ],
   "source": [
    "LSTM_HIDDEN = 128\n",
    "LSTM_LAYERS = 1\n",
    "\n",
    "model = Model(embedding_matrix, lstm_hidden=LSTM_HIDDEN, lstm_layers=LSTM_LAYERS).to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84a06d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e629c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for text_ids, extra_feats, labels in tqdm(dataloader, desc='Epoch training'):\n",
    "        text_ids = text_ids.to(device)\n",
    "        extra_feats = extra_feats.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(text_ids, extra_feats)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "\n",
    "    return total_loss / len(dataloader), acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e61afe",
   "metadata": {},
   "source": [
    "### Evaluate config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f8a648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('../data/preprocessed/val.csv')\n",
    "val_dataset = SentimentDataset(val_df, word2idx, max_len=MAX_LEN)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4cc3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text_ids, extra_feats, labels in tqdm(dataloader, desc='Validating'):\n",
    "            text_ids = text_ids.to(device)\n",
    "            extra_feats = extra_feats.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(text_ids, extra_feats)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = correct / total\n",
    "\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69879e4",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ee02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "history_train_acc = []\n",
    "history_train_loss = []\n",
    "history_val_acc = []\n",
    "history_val_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    history_train_loss.append(train_loss)\n",
    "    history_train_acc.append(train_acc)\n",
    "\n",
    "    val_loss, val_acc = evaluate(\n",
    "        model,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    history_val_loss.append(val_loss)\n",
    "    history_val_acc.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f}\"\n",
    "        \"\\n--------------------------------------------\\n\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
