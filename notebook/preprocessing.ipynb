{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a5cf2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../data/raw/goemotions_1.csv')\n",
    "df2 = pd.read_csv('../data/raw/goemotions_2.csv')\n",
    "df3 = pd.read_csv('../data/raw/goemotions_3.csv')\n",
    "\n",
    "df = pd.concat([df1, df2, df3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0c4fd",
   "metadata": {},
   "source": [
    "## Labels mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7ca3b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/mapping/sentiment_dict.json', mode='r', encoding='utf-8') as f:\n",
    "    map_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b20a9e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': ['amusement',\n",
       "  'excitement',\n",
       "  'joy',\n",
       "  'love',\n",
       "  'desire',\n",
       "  'optimism',\n",
       "  'caring',\n",
       "  'pride',\n",
       "  'admiration',\n",
       "  'gratitude',\n",
       "  'relief',\n",
       "  'approval'],\n",
       " 'negative': ['fear',\n",
       "  'nervousness',\n",
       "  'remorse',\n",
       "  'embarrassment',\n",
       "  'disappointment',\n",
       "  'sadness',\n",
       "  'grief',\n",
       "  'disgust',\n",
       "  'anger',\n",
       "  'annoyance',\n",
       "  'disapproval'],\n",
       " 'ambiguous': ['realization', 'surprise', 'curiosity', 'confusion']}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6e681e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = df.drop(columns=['rater_id']).select_dtypes(int).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ed109ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_3classes(row):\n",
    "    labels_on = set(label for label in label_cols if row[label] == 1)\n",
    "\n",
    "    has_pos = len(labels_on & set(map_data['positive'])) > 0\n",
    "    has_neg = len(labels_on & set(map_data['negative'])) > 0\n",
    "\n",
    "    if has_neg:\n",
    "        return 2\n",
    "    elif has_pos:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df.apply(map_to_3classes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29188782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    79349\n",
       "0    77078\n",
       "2    54798\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e8e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_unique_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"If you don't wear BROWN AND ORANGE...YOU DON...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"What do Scottish people look like?\" How I wo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### A surprise, to be sure, but a welcome one</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'*Pray*, v. To ask that the laws of the unive...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;it'll get invaded by tankie, unfortunately. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57727</th>\n",
       "      <td>ü§∑üèª‚Äç‚ôÄÔ∏è As a wise man once said: he was a bastar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57728</th>\n",
       "      <td>ü§∑üèº‚Äç‚ôÄÔ∏è I was wondering the same thing. It looke...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57729</th>\n",
       "      <td>ü¶ÄMY BABYS DEADü¶Ä</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57730</th>\n",
       "      <td>ü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶ÄI‚Äôm bad at this gameü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Ä</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57731</th>\n",
       "      <td>üßÄ &lt;‚Äî‚ÄîThe best gold for you! üôå</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57732 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  n_unique_sentiments\n",
       "0       \"If you don't wear BROWN AND ORANGE...YOU DON...                    3\n",
       "1       \"What do Scottish people look like?\" How I wo...                    2\n",
       "2         ### A surprise, to be sure, but a welcome one                     1\n",
       "3       '*Pray*, v. To ask that the laws of the unive...                    1\n",
       "4       >it'll get invaded by tankie, unfortunately. ...                    1\n",
       "...                                                  ...                  ...\n",
       "57727  ü§∑üèª‚Äç‚ôÄÔ∏è As a wise man once said: he was a bastar...                    2\n",
       "57728  ü§∑üèº‚Äç‚ôÄÔ∏è I was wondering the same thing. It looke...                    2\n",
       "57729                                    ü¶ÄMY BABYS DEADü¶Ä                    2\n",
       "57730  ü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶ÄI‚Äôm bad at this gameü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Äü¶Ä                    1\n",
       "57731                      üßÄ <‚Äî‚ÄîThe best gold for you! üôå                    2\n",
       "\n",
       "[57732 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_label_count = (\n",
    "    df.groupby('text')['sentiment']\n",
    "      .nunique()\n",
    "      .reset_index(name='n_unique_sentiments')\n",
    ")\n",
    "\n",
    "group_label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_texts = group_label_count[group_label_count['n_unique_sentiments'] > 1]['text']\n",
    "conflict_df = df[df['text'].isin(conflict_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab01b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5758504815353703"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conflict_texts) / df['text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e0919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "[0, 1]       12481\n",
       "[1, 2]       10484\n",
       "[0, 1, 2]     5933\n",
       "[0, 2]        4347\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conflict_df.groupby('text')['sentiment'].apply(lambda x: sorted(x.unique())).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ed82a",
   "metadata": {},
   "source": [
    "## Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def resolve_group(sentiments):\n",
    "    c = Counter(sentiments)\n",
    "    total = sum(c.values())\n",
    "\n",
    "    labels = set(c.keys())\n",
    "\n",
    "    ratios = {k: v / total for k, v in c.items()}\n",
    "    max_label, max_ratio = max(ratios.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Case A: POS/NEU or NEU/NEG\n",
    "    if labels in ({0, 1}, {1, 2}):\n",
    "        if max_ratio > 0.5:\n",
    "            return max_label, False, max_ratio\n",
    "        else:\n",
    "            return None, True, max_ratio\n",
    "\n",
    "    # Case B: POS vs NEG\n",
    "    if labels == {0, 2}:\n",
    "        if max_ratio >= 0.7:\n",
    "            return max_label, False, max_ratio\n",
    "        else:\n",
    "            return None, True, max_ratio\n",
    "\n",
    "    # Case C: POS vs NEU vs NEG\n",
    "    if labels == {0, 1, 2}:\n",
    "        if max_ratio >= 0.5:\n",
    "            return max_label, False, max_ratio\n",
    "        else:\n",
    "            return None, True, max_ratio\n",
    "\n",
    "    # No conflict (single label)\n",
    "    return max_label, False, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc175972",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved = df.groupby('text')['sentiment'].apply(resolve_group).reset_index()\n",
    "resolved[['sentiment', 'is_uncertain', 'confidence']] = pd.DataFrame(resolved['sentiment'].tolist(), index=resolved.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c4e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_df = resolved[resolved['is_uncertain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9c465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>is_uncertain</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"If you don't wear BROWN AND ORANGE...YOU DON...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Calm down and relax are the worst things to s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Luckily from him, there is no death penalty i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>No way, man. We're gonna keep on rockin' fore...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>cute, new driver still cares. Come back in 4 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57704</th>\n",
       "      <td>üòÇüòÇüòÇüòÇ please [NAME] tell me that was in Titusvi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>üòÖ sorry. Lol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57713</th>\n",
       "      <td>üò• I feel so sorry. He'd be proud of you. *Hugs*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57714</th>\n",
       "      <td>üò≠ I wanted her to be on the top 4; now we‚Äôve h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57721</th>\n",
       "      <td>ü§£ wow... I read it all in a serious tone until...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6645 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment  \\\n",
       "0       \"If you don't wear BROWN AND ORANGE...YOU DON...        NaN   \n",
       "8       Calm down and relax are the worst things to s...        NaN   \n",
       "19      Luckily from him, there is no death penalty i...        NaN   \n",
       "22      No way, man. We're gonna keep on rockin' fore...        NaN   \n",
       "37      cute, new driver still cares. Come back in 4 ...        NaN   \n",
       "...                                                  ...        ...   \n",
       "57704  üòÇüòÇüòÇüòÇ please [NAME] tell me that was in Titusvi...        NaN   \n",
       "57708                                       üòÖ sorry. Lol        NaN   \n",
       "57713    üò• I feel so sorry. He'd be proud of you. *Hugs*        NaN   \n",
       "57714  üò≠ I wanted her to be on the top 4; now we‚Äôve h...        NaN   \n",
       "57721  ü§£ wow... I read it all in a serious tone until...        NaN   \n",
       "\n",
       "       is_uncertain  confidence  \n",
       "0              True    0.400000  \n",
       "8              True    0.600000  \n",
       "19             True    0.400000  \n",
       "22             True    0.666667  \n",
       "37             True    0.400000  \n",
       "...             ...         ...  \n",
       "57704          True    0.666667  \n",
       "57708          True    0.333333  \n",
       "57713          True    0.666667  \n",
       "57714          True    0.666667  \n",
       "57721          True    0.333333  \n",
       "\n",
       "[6645 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536a5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1.0    0.407618\n",
       "0.0    0.362323\n",
       "2.0    0.230059\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolved[resolved['sentiment'].notna()].sentiment.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f855b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved.to_csv('../data/log/rule_labeled.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ca2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.special import softmax\n",
    "import urllib.request\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "task = 'sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "labels = []\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.to(device)         \n",
    "model.eval()\n",
    "\n",
    "def predict(text):\n",
    "  encoded_input = tokenizer(text, return_tensors='pt')\n",
    "  encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "  with torch.no_grad():\n",
    "      output = model(**encoded_input)\n",
    "\n",
    "  scores = output.logits[0].cpu().numpy()\n",
    "  scores = softmax(scores)\n",
    "\n",
    "  return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec443c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LABEL_TO_DATASET_LABEL = np.array([2, 1, 0])\n",
    "MARGIN = 0.3\n",
    "\n",
    "def auto_label(text: str) -> pd.Series:\n",
    "    scores = np.asarray(predict(text), dtype=float)\n",
    "\n",
    "    top1 = scores.argmax()\n",
    "    top2 = np.partition(scores, -2)[-2:]\n",
    "    gap = top2.max() - top2.min()\n",
    "\n",
    "    if gap > MARGIN:\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"sentiment\": int(MODEL_LABEL_TO_DATASET_LABEL[top1]),\n",
    "                \"is_uncertain\": False,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"sentiment\": pd.NA,\n",
    "            \"is_uncertain\": True,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c24377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6645/6645 [01:43<00:00, 64.21it/s]\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_8580\\2015681362.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[<NA> 2 1 ... <NA> 2 0]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  resolved.loc[mask, ['sentiment', 'is_uncertain']] = (\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "mask = resolved['is_uncertain']\n",
    "\n",
    "resolved.loc[mask, ['sentiment', 'is_uncertain']] = (\n",
    "    resolved.loc[mask, 'text']\n",
    "    .progress_apply(auto_label)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac458bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved.to_csv('../data/log/soft_labeled.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1a64b",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "06e025c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/labeled/total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4ee4538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val_test, _, _ = train_test_split(df, df.sentiment, test_size=0.3, random_state=42)\n",
    "df_val, df_test, _, _ = train_test_split(df_val_test, df_val_test.sentiment, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d249f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/labeled/train.csv', encoding='utf-8', index=False)\n",
    "df_val.to_csv('../data/labeled/val.csv', encoding='utf-8', index=False)\n",
    "df_test.to_csv('../data/labeled/test.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143c100",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c8cfbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../data/labeled/train.csv', encoding='utf-8')\n",
    "val_df = pd.read_csv('../data/labeled/val.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "50f03fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>@teamYouTube ok and thanks',\n",
       " 'Replace that friend with a whole pile of desillusioned friends and you might as well @me',\n",
       " '@lies_about_flossing, sorry if I used the wrong pronoun.',\n",
       " 'Reach out to @teamyoutube on twitter and explain what is going on.']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_mention = train_df[\"text\"].str.contains(r\"(?<!\\w)@[A-Za-z_][A-Za-z0-9_]{1,30}\", regex=True, na=False)\n",
    "\n",
    "train_df[has_mention].text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3cdbfaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"> Update they would've never been arrested or charged if they weren't black. Epitome of the disgusting systemic racism in this country #UBERPOOLtoPrisonPipeline\",\n",
       " '#I BELIEVE IN [NAME]',\n",
       " '#YES',\n",
       " 'no, mostly just spurred from mild boredom and thinking of the [NAME] future. Getting a gauge at the caliber of players chosen at #12.',\n",
       " 'No thank you, we need to tank, and this moves us 1 gb from the #6 pick :)']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_hashtag = train_df[\"text\"].str.contains(r\"#\\w[\\w-]*\", regex=True, na=False)\n",
    "\n",
    "train_df[has_hashtag].text.to_list()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e81c5",
   "metadata": {},
   "source": [
    "### Contraction mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a0265e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "contraction_df = pd.read_csv(\"../data/mapping/contraction.csv\", encoding='utf-8')\n",
    "contraction_map = {key: value for key, value in zip(contraction_df.contraction, contraction_df.extension)}\n",
    "\n",
    "contraction_pattern = re.compile(\n",
    "    r\"\\b(\" + \"|\".join(map(re.escape, contraction_map.keys())) + r\")\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def replace_contraction(match):\n",
    "    w = match.group(0).lower()\n",
    "    if w in contraction_map:\n",
    "        return contraction_map[match.group(0).lower()]\n",
    "    else:\n",
    "        return w\n",
    "\n",
    "def extend(text):\n",
    "    return re.sub(contraction_pattern, replace_contraction, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4fbf0f",
   "metadata": {},
   "source": [
    "### Emoji mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "286ed93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emoji\n",
    "\n",
    "emoji_df = pd.read_csv('../data/mapping/emoji.csv')\n",
    "emoji_df['Score'] = np.tanh(\n",
    "    np.log((emoji_df.Positive + 1) / (emoji_df.Negative + 1))\n",
    ")\n",
    "\n",
    "GENDER_EMOJI_MAP = {\n",
    "    \"\\u2640\": \"[EMO_FEMALE]\",  # ‚ôÄ\n",
    "    \"\\u2642\": \"[EMO_MALE]\",    # ‚ôÇ\n",
    "    \"\\u26A7\": \"[EMO_TRANS]\"    # ‚öß\n",
    "}\n",
    "\n",
    "def normalize_emoji(e):\n",
    "    e = re.sub(r\"\\uFE0F\", \"\", e)\n",
    "    if e in GENDER_EMOJI_MAP:\n",
    "        return GENDER_EMOJI_MAP[e]\n",
    "    e = re.sub(r\"\\u200d\", \"\", e)\n",
    "    return e\n",
    "\n",
    "emoji_df['Emoji_norm'] = emoji_df['Emoji'].apply(normalize_emoji)\n",
    "emoji_df = emoji_df[emoji_df['Emoji_norm'].str.len() > 0]\n",
    "\n",
    "emoji_list = sorted(\n",
    "    emoji_df['Emoji_norm'].unique(),\n",
    "    key=len,\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "emoji_map = dict(\n",
    "    zip(\n",
    "        emoji_df['Emoji_norm'],\n",
    "        zip(emoji_df['Unicode name'], emoji_df.Score)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9c65a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emoji(sentence, beta=1.0):\n",
    "    emoji_scores = []\n",
    "    new_text = sentence\n",
    "    strongest = 0.0\n",
    "\n",
    "    for e in emoji.emoji_list(sentence):\n",
    "        em = e['emoji']\n",
    "        norm_em = normalize_emoji(em)\n",
    "\n",
    "        if not norm_em.startswith(\"[EMO_\"):\n",
    "            name, score = emoji_map.get(norm_em, ('[EMO]', 0.0))\n",
    "            if score != 0.0:\n",
    "                emoji_scores.append(score)\n",
    "        else:\n",
    "            name = norm_em\n",
    "\n",
    "        new_text = new_text.replace(em, name)\n",
    "\n",
    "    if emoji_scores:\n",
    "        strongest = max(emoji_scores, key=lambda s: abs(s))\n",
    "\n",
    "    return new_text, strongest * beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ffd44",
   "metadata": {},
   "source": [
    "### Markdown patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "26167cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_11416\\4219348763.py:17: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask = train_df[\"text\"].str.contains(pattern, regex=True, na=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bold': {'exists': np.True_, 'count': np.int64(103)},\n",
       " 'italic_star': {'exists': np.True_, 'count': np.int64(566)},\n",
       " 'italic_underscore': {'exists': np.True_, 'count': np.int64(27)},\n",
       " 'bold_italic': {'exists': np.True_, 'count': np.int64(14)},\n",
       " 'strikethrough': {'exists': np.True_, 'count': np.int64(42)},\n",
       " 'inline_code': {'exists': np.True_, 'count': np.int64(1)},\n",
       " 'code_block': {'exists': np.False_, 'count': np.int64(0)},\n",
       " 'quote': {'exists': np.True_, 'count': np.int64(508)},\n",
       " 'spoiler': {'exists': np.True_, 'count': np.int64(18)},\n",
       " 'link': {'exists': np.False_, 'count': np.int64(0)}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = {\n",
    "    \"bold\": r\"\\*\\*.+?\\*\\*\",\n",
    "    \"italic_star\": r\"\\*(?!\\*)(.+?)(?<!\\*)\\*\",\n",
    "    \"italic_underscore\": r\"_(.+?)_\",\n",
    "    \"bold_italic\": r\"\\*\\*\\*.+?\\*\\*\\*\",\n",
    "    \"strikethrough\": r\"~~.+?~~\",\n",
    "    \"inline_code\": r\"`.+?`\",\n",
    "    \"code_block\": r\"```[\\s\\S]+?```\",\n",
    "    \"quote\": r\"^>.+\",\n",
    "    \"spoiler\": r\">!.+!<\",\n",
    "    \"link\": r\"\\[.+?\\]\\(.+?\\)\"\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, pattern in patterns.items():\n",
    "    mask = train_df[\"text\"].str.contains(pattern, regex=True, na=False)\n",
    "    results[name] = {\n",
    "        \"exists\": mask.any(),\n",
    "        \"count\": mask.sum()\n",
    "    }\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "639801f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_markdown(text):\n",
    "    # spoiler\n",
    "    text = re.sub(r\">!(.+?)!<\", r\" <spoiler> \\1 </spoiler> \", text)\n",
    "\n",
    "    # bold + italic\n",
    "    text = re.sub(r\"\\*\\*\\*(.+?)\\*\\*\\*\", r\" <bi> \\1 </bi> \", text)\n",
    "\n",
    "    # bold\n",
    "    text = re.sub(r\"\\*\\*(.+?)\\*\\*\", r\" <b> \\1 </b> \", text)\n",
    "\n",
    "    # italic *\n",
    "    text = re.sub(r\"\\*(?!\\*)(.+?)(?<!\\*)\\*\", r\" <i> \\1 </i> \", text)\n",
    "\n",
    "    # strike\n",
    "    text = re.sub(r\"~~(.+?)~~\", r\" <s> \\1 </s> \", text)\n",
    "\n",
    "    # quote (line-based)\n",
    "    text = re.sub(r\"^>(.+)\", r\" <q> \\1 </q> \", text, flags=re.M)\n",
    "\n",
    "    # inline code\n",
    "    text = re.sub(r\"`(.+?)`\", r\" <code> \\1 </code> \", text)\n",
    "\n",
    "    # triple double quotes\n",
    "    text = re.sub(r'\"\"\"\\s*(.+?)\\s*\"\"\"', r' <quote> \\1 </quote> ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be7621",
   "metadata": {},
   "source": [
    "### Mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "52839126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mention(text):\n",
    "    return re.sub(r\"(?<!\\w)@[A-Za-z_][A-Za-z0-9_]{1,30}\", '[MENTION]', text)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c9786",
   "metadata": {},
   "source": [
    "### URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d56a4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_url(text):\n",
    "    return re.sub(r\"https?://\\S+|www\\.\\S+\", '[URL]', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9109c",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4a0a4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_time(text):\n",
    "    return re.sub(r\"\\b(?:1[0-2]|0?[1-9]):[0-5][0-9]\\s*(?i:am|pm)\\b\", '[TIME]', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb69c9",
   "metadata": {},
   "source": [
    "### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "221e7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_date(text):\n",
    "    # ISO 8601 datetime: 2026-01-07T10:30:00\n",
    "    text = re.sub(r\"\\b\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\b\", \"[DATE]\", text)\n",
    "    # YYYY-MM-DD\n",
    "    text = re.sub(r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\", \"[DATE]\", text)\n",
    "    # MM/DD/YYYY\n",
    "    text = re.sub(r\"\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b\", \"[DATE]\", text)\n",
    "    # DD-MM-YYYY\n",
    "    text = re.sub(r\"\\b\\d{1,2}-\\d{1,2}-\\d{4}\\b\", \"[DATE]\", text)\n",
    "    # Month Day, Year (Jan 7, 2026 or January 7, 2026)\n",
    "    text = re.sub(\n",
    "        r\"\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|\"\n",
    "        r\"January|February|March|April|May|June|July|August|September|October|November|December)\"\n",
    "        r\"\\s+\\d{1,2},\\s*\\d{4}\\b\", \"[DATE]\", text\n",
    "    )\n",
    "    # Day Month Year (7 Jan 2026 or 7 January 2026)\n",
    "    text = re.sub(\n",
    "        r\"\\b\\d{1,2}\\s+\"\n",
    "        r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|\"\n",
    "        r\"January|February|March|April|May|June|July|August|September|October|November|December)\"\n",
    "        r\"\\s+\\d{4}\\b\", \"[DATE]\", text\n",
    "    )\n",
    "    # Compact numeric YYYYMMDD\n",
    "    text = re.sub(r\"\\b\\d{8}\\b\", \"[DATE]\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f279c84",
   "metadata": {},
   "source": [
    "### Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a581a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_hashtag(text):\n",
    "\n",
    "    def repl(m):\n",
    "        tag = m.group()[1:]\n",
    "        tag = tag.lower()\n",
    "        return f\"[HASHTAG] {tag}\"\n",
    "    \n",
    "    return re.sub(r'#\\w+', repl, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e34aad",
   "metadata": {},
   "source": [
    "### Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "36ff1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c5bc06",
   "metadata": {},
   "source": [
    "### Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ce864d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    token_pattern = r'(\\[[A-Z_]+\\])'\n",
    "\n",
    "    parts = re.split(token_pattern, text)\n",
    "\n",
    "    parts = [p.lower() if not re.fullmatch(token_pattern, p) else p for p in parts]\n",
    "\n",
    "    return ''.join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc63fac",
   "metadata": {},
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e9a7e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_punctuation(text: str) -> str:\n",
    "    text = re.sub(r'\\.{3,}', '...', text)\n",
    "\n",
    "    text = re.sub(r'!{3,}', '!!', text)\n",
    "    text = re.sub(r'\\?{3,}', '??', text)\n",
    "\n",
    "    text = re.sub(r'(!\\?|\\?!){2,}', '!?', text)\n",
    "\n",
    "    text = re.sub(r'\\s+([!?.,])', r'\\1', text)\n",
    "    text = re.sub(r'([!?.,])\\s+', r'\\1 ', text)\n",
    "\n",
    "    text = re.sub(r'([,;:]){2,}', r'\\1', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd972e03",
   "metadata": {},
   "source": [
    "### Feature uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d2bc4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_is_all_uppercase(text):\n",
    "    return text, int(text.isupper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ebeed0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_uppercase_ratio(text):\n",
    "    clean_text = re.sub(r'\\[[A-Z_]+\\]', '', text)\n",
    "    \n",
    "    alphas = [c for c in clean_text if c.isalpha()]\n",
    "    \n",
    "    if not alphas:\n",
    "        return text,0.0\n",
    "    \n",
    "    return text, sum(c.isupper() for c in alphas) / len(alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7160892",
   "metadata": {},
   "source": [
    "### Feature exclaimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fcd1861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exclamation_intensity(text, cap=5):\n",
    "    max_run = 0\n",
    "    cur = 0\n",
    "    for c in text:\n",
    "        if c == '!':\n",
    "            cur += 1\n",
    "            max_run = max(max_run, cur)\n",
    "        else:\n",
    "            cur = 0\n",
    "\n",
    "    return text, min(max_run, cap) / cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a579a2",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "216fdba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"***aAa*** HATE? @abc123 ijiqi aikks....... !!!! üòî this's http://localhost:8080/api https://google.com 2:00 AM 01/01/2026 ??? #NLP ###\", 0.8)\n",
      "('<bi> aAa </bi> HATE? [MENTION] ijiqi aikks....... !!!! [EMO_SAD] this is [URL] [URL] [TIME] [DATE] ??? [HASHTAG] nlp ###', 0.16666666666666666)\n",
      "<bi> aaa </bi> hate? [MENTION] ijiqi aikks...!! [EMO_SAD] this is [URL] [URL] [TIME] [DATE]?? [HASHTAG] nlp ### -0.36035535813436975\n"
     ]
    }
   ],
   "source": [
    "text = \"***aAa*** HATE? @abc123 ijiqi aikks....... !!!! üòî this's http://localhost:8080/api https://google.com 2:00 AM 01/01/2026 ??? #NLP ###\"\n",
    "\n",
    "print(extract_exclamation_intensity(text, cap=5))\n",
    "\n",
    "text = extract_markdown(text)\n",
    "text = extend(text)\n",
    "text, score = extract_emoji(text, beta=1.0)\n",
    "text = normalize_mention(text)\n",
    "text = normalize_url(text)\n",
    "text = normalize_time(text)\n",
    "text = normalize_date(text)\n",
    "text = normalize_hashtag(text)\n",
    "text = normalize_whitespace(text)\n",
    "\n",
    "print(extract_uppercase_ratio(text))\n",
    "\n",
    "text = lowercase(text)\n",
    "text = normalize_punctuation(text)\n",
    "print(text, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cf4ec",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "54eb7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    patterns = [\n",
    "        r\"\\[[A-Z_]+\\]\",\n",
    "        r\"<\\/?[\\w_]+>\",\n",
    "        r\"\\w+\",\n",
    "        r\"[?!]{2,}\",\n",
    "        r\"\\.{3,}\",\n",
    "        r\"[^\\w\\s]\"\n",
    "    ]\n",
    "\n",
    "    combined = re.compile(\"|\".join(patterns), re.UNICODE)\n",
    "\n",
    "    return combined.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa0c2fe",
   "metadata": {},
   "source": [
    "### Pipeline order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b2b55dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "preprocessing_pipeline = [\n",
    "    (partial(extract_exclamation_intensity, cap=5), 'ex_intensity'),\n",
    "    (extract_markdown),\n",
    "    (extend),\n",
    "    (partial(extract_emoji, beta=1.0), 'emoji_score'),\n",
    "    (normalize_mention),\n",
    "    (normalize_url),\n",
    "    (normalize_time),\n",
    "    (normalize_date),\n",
    "    (normalize_hashtag),\n",
    "    (normalize_whitespace),\n",
    "    (extract_is_all_uppercase, 'all_uppercase'),\n",
    "    (extract_uppercase_ratio, 'uppercase_ratio'),\n",
    "    (lowercase),\n",
    "    (normalize_punctuation)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "343be331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(text, pipeline=preprocessing_pipeline):\n",
    "    res = {\"text\": text}\n",
    "\n",
    "    for process in pipeline:\n",
    "\n",
    "        if isinstance(process, tuple):\n",
    "            func, key = process\n",
    "            res['text'], res[key] = func(res['text'])\n",
    "        else:\n",
    "            func = process\n",
    "            res['text'] = func(res['text'])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9b885d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<bi> aaa </bi> hate? [MENTION] ijiqi aikks...!! [EMO_SAD] this is [URL] [URL] [TIME] [DATE]?? [HASHTAG] nlp',\n",
       " 'ex_intensity': 0.8,\n",
       " 'emoji_score': -0.36035535813436975,\n",
       " 'all_uppercase': 0,\n",
       " 'uppercase_ratio': 0.16666666666666666}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_preprocessing(\"***aAa*** HATE? @abc123 ijiqi aikks....... !!!! üòî this's http://localhost:8080/api https://google.com 2:00 AM 01/01/2026 ??? #NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5896d5f",
   "metadata": {},
   "source": [
    "## Apply pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0d7491bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df = pd.DataFrame(train_df.text.apply(apply_preprocessing).to_list())\n",
    "processed_train_df['label'] = train_df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4b899233",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_val_df = pd.DataFrame(val_df.text.apply(apply_preprocessing).to_list())\n",
    "processed_val_df['label'] = val_df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "aaf08813",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df.to_csv(\"../data/preprocessed/train.csv\", encoding='utf-8', index=False)\n",
    "processed_val_df.to_csv(\"../data/preprocessed/val.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
